{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531ac726-8a34-4d92-8695-7d2cd6a15572",
   "metadata": {},
   "source": [
    "# Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406367ea-8ca9-4643-8b8d-1c3ce1b32261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.elastic import ArticleSearchQuery\n",
    "from modules.objects import FullArticle\n",
    "from modules.config import BaseConfig\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logger = logging.getLogger(\"osinter\")\n",
    "load_dotenv()\n",
    "\n",
    "config_options = BaseConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780feb50-852f-4609-ab6e-94fff1df12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from openai import OpenAIError, OpenAI\n",
    "from openai.types.chat import ChatCompletion, ChatCompletionMessageParam\n",
    "import tiktoken\n",
    "\n",
    "from tenacity import (\n",
    "    RetryError,\n",
    "    before_sleep_log,\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "openai_client = OpenAI(api_key=config_options.OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929f83a-b967-4767-a5d7-0f84b70c8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"You are given a summary of a news article, surrounded by triple qoutes, and your only job is to state if the article describes a specific cyber security incident.\n",
    "Clarifying the Definition of \"Incident\": Emphasize that the incident must be explicitly described with clear evidence of a specific event occurring, rather than potential or evaded threats.\n",
    "THERE MUST BE IMPACT! NO POTENTIAL IMPACT! IT MUST HAVE HAPPENED! Use the CIA triad to decide on impact\n",
    "You shall only return a number, and nothing else. If the article descripes an incident, you should return a 2, and if not return a 1\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce833-ae41-414d-96ad-4f5a48a95292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(prompts: list[ChatCompletionMessageParam]) -> str | None:\n",
    "    @retry(\n",
    "        wait=wait_random_exponential(min=1, max=3600),\n",
    "        stop=stop_after_attempt(20),\n",
    "        retry=retry_if_exception_type(OpenAIError),\n",
    "        before_sleep=before_sleep_log(logger, logging.DEBUG),\n",
    "    )\n",
    "    def query(q: list[ChatCompletionMessageParam]) -> ChatCompletion:\n",
    "        return openai_client.chat.completions.create(\n",
    "            model=\"ft:gpt-3.5-turbo-1106:osinter-bertie:incident:8wsHrdRo\",\n",
    "            messages=q,\n",
    "            n=1,\n",
    "            temperature=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        return query(prompts).choices[0].message.content\n",
    "    except RetryError:\n",
    "        return None\n",
    "\n",
    "def is_incident(content: str) -> bool | str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction_prompt},\n",
    "        {\"role\": \"user\", \"content\": f'\"\"\"{content}\"\"\"'}\n",
    "    ]\n",
    "\n",
    "    response = query_openai(messages)\n",
    "\n",
    "    try:\n",
    "        number = int(response)\n",
    "        if number < 1 or number > 2:\n",
    "            return response\n",
    "        else:\n",
    "            return number == 2\n",
    "    except ValueError:\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fd1f4-7009-4d0b-b524-bc330122c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self) -> None:\n",
    "        self.lock = multiprocessing.Manager().Lock()\n",
    "        self.count = 0\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        with self.lock:\n",
    "            self.count += 1\n",
    "            return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8257f-9bee-4847-ae1a-bad2fd198116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config_options.ELASTICSEARCH_ARTICLE_INDEX)\n",
    "articles = config_options.es_article_client.query_all_documents()\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2805d0-86d3-453e-ac42-f65ce9ba73bf",
   "metadata": {},
   "source": [
    "# For validation and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853953fd-f581-47dd-9ca1-39b098a8d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_classified_articles = [article for article in articles if article.ml.incident and article.summary]\n",
    "incident = [article for article in pre_classified_articles if article.ml.incident == 2]\n",
    "not_incident = [article for article in pre_classified_articles if article.ml.incident == 1]\n",
    "\n",
    "shortest = min(len(incident), len(not_incident))\n",
    "dataset = []\n",
    "dataset.extend(incident[:shortest])\n",
    "dataset.extend(not_incident[:shortest])\n",
    "\n",
    "\n",
    "print(\n",
    "    len(pre_classified_articles),\n",
    "    len(incident),\n",
    "    len(not_incident),\n",
    "    len(dataset),\n",
    "    {article.ml.incident for article in articles}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbd29b-249e-4cbd-af2a-f61669f1adc5",
   "metadata": {},
   "source": [
    "## For finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e3a60-ba24-4688-b688-48ff9bf92fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "for article in dataset:\n",
    "    prompts.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": instruction_prompt},\n",
    "            {\"role\": \"user\", \"content\": f'\"\"\"{i.summary}\"\"\"'},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{i.ml.incident}\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "with open(\"finetuning.jsonl\", \"w\") as f:\n",
    "    for prompt in prompts:\n",
    "        f.write(json.dumps(prompt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394efa7d-544d-4869-9bd2-79e661fc21fc",
   "metadata": {},
   "source": [
    "## For validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4be5cf-431b-4169-8893-ba2981dbee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "successes = []\n",
    "failures = []\n",
    "counter = Counter()\n",
    "\n",
    "def validate_article(article):\n",
    "    count = counter.get_count()\n",
    "    print(f\"Starting {count}\")\n",
    "    response = is_incident(article.summary)\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        failures.append((article, response))\n",
    "    else:\n",
    "        if response == (article.ml.incident == 2):\n",
    "            successes.append(article)\n",
    "        else:\n",
    "            failures.append((article, response))\n",
    "    \n",
    "    print(f\"Stopped {count}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "    executor.map(validate_article, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df0799-0ffa-41ec-9f8d-3a37058b07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_fail = [fail for fail in failures if isinstance(fail[1], str)]\n",
    "incident_fail = [fail for fail in failures if fail[0].ml.incident == 2]\n",
    "not_incident_fail = [fail for fail in failures if fail[0].ml.incident == 1]\n",
    "\n",
    "print(len(successes), len(failures), len(formatting_fail), len(incident_fail), len(not_incident_fail))\n",
    "\n",
    "for fail in failures:\n",
    "    article = fail[0]\n",
    "\n",
    "    is_incident = \"Incident\" if article.ml.incident == 2 else \"Not-incident\"\n",
    "    print(f\"{is_incident}: {article.summary}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207be6ad-0032-4b51-87c2-d41a0aa786c5",
   "metadata": {},
   "source": [
    "# For prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd0521-f86d-4ce7-aee0-514c8575503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    count = counter.get_count()\n",
    "    print(f\"Starting {count}\")\n",
    "    response = query_for_incident(article.summary)\n",
    "\n",
    "    if isinstance(response, bool):\n",
    "        article.ml.classification.incident = response\n",
    "    \n",
    "    print(f\"Stopped {count}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "    executor.map(process_article, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ba33e-48b0-474c-b625-5129bac308a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"./classified.gz\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    json.dump([article.model_dump(mode=\"json\") for article in articles], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896a679-7663-4b6f-8211-0203bb01a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_options.es_article_client.update_documents(articles, [\"ml\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
